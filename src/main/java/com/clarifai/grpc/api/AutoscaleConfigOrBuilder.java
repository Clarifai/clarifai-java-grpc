// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: proto/clarifai/api/resources.proto

package com.clarifai.grpc.api;

public interface AutoscaleConfigOrBuilder extends
    // @@protoc_insertion_point(interface_extends:clarifai.api.AutoscaleConfig)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <pre>
   * The minimum number of replicas for the runner to have.
   * Defaults to 0 which means autoscaling can scale down to zero.
   * If you want a replica always up then set to &gt;= 1.
   * </pre>
   *
   * <code>uint32 min_replicas = 1;</code>
   * @return The minReplicas.
   */
  int getMinReplicas();

  /**
   * <pre>
   * The maximium number of replicas to scale up the runner to.
   * </pre>
   *
   * <code>uint32 max_replicas = 2;</code>
   * @return The maxReplicas.
   */
  int getMaxReplicas();

  /**
   * <pre>
   * The number of seconds of traffic history to consider when autoscaling.
   * </pre>
   *
   * <code>uint32 traffic_history_seconds = 3;</code>
   * @return The trafficHistorySeconds.
   */
  int getTrafficHistorySeconds();

  /**
   * <pre>
   * The time to wait before scaling down after the last request.
   * </pre>
   *
   * <code>uint32 scale_down_delay_seconds = 4;</code>
   * @return The scaleDownDelaySeconds.
   */
  int getScaleDownDelaySeconds();

  /**
   * <pre>
   * The time to wait between scaling up replicas without burst traffic.
   * </pre>
   *
   * <code>uint32 scale_up_delay_seconds = 5;</code>
   * @return The scaleUpDelaySeconds.
   */
  int getScaleUpDelaySeconds();

  /**
   * <pre>
   * Depending on your plan you may be able to enable packing of resources into a single node
   * for more compute and cost efficiency.
   * </pre>
   *
   * <code>bool disable_packing = 7;</code>
   * @return The disablePacking.
   */
  boolean getDisablePacking();
}
