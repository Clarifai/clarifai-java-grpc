// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: proto/clarifai/api/resources.proto

package com.clarifai.grpc.api;

public interface AndOrBuilder extends
    // @@protoc_insertion_point(interface_extends:clarifai.api.And)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <pre>
   * FILTER by input.data... information.
   * This can include human provided concepts, geo location info, metadata, etc.
   * This is effectively searching over only the trusted annotation attached to an input in your
   * app. To search by more specific annotation fields use the Annotation object here.
   * </pre>
   *
   * <code>.clarifai.api.Input input = 1;</code>
   */
  boolean hasInput();
  /**
   * <pre>
   * FILTER by input.data... information.
   * This can include human provided concepts, geo location info, metadata, etc.
   * This is effectively searching over only the trusted annotation attached to an input in your
   * app. To search by more specific annotation fields use the Annotation object here.
   * </pre>
   *
   * <code>.clarifai.api.Input input = 1;</code>
   */
  com.clarifai.grpc.api.Input getInput();
  /**
   * <pre>
   * FILTER by input.data... information.
   * This can include human provided concepts, geo location info, metadata, etc.
   * This is effectively searching over only the trusted annotation attached to an input in your
   * app. To search by more specific annotation fields use the Annotation object here.
   * </pre>
   *
   * <code>.clarifai.api.Input input = 1;</code>
   */
  com.clarifai.grpc.api.InputOrBuilder getInputOrBuilder();

  /**
   * <pre>
   * RANK based predicted outputs from models such as custom trained models, pre-trained models,
   * etc. This is also where you enter the image url for a visual search because what we're asking
   * the system to do is find output embedding most visually similar to the provided input (that
   * input being in And.output.input.data.image.url for example). This will return the Hits
   * sorted by visual similarity (1.0 being very similar or exact match and 0.0 being very
   * dissimlar). For a search by Output concept, this means we're asking the system to rank
   * the Hits by confidence of our model's predicted Outputs. So for example if the model
   * predicts an image is 0.95 likely there is a "dog" present, that should related directly
   * to the score returned if you search for Output concept "dog" in your query. This provides
   * a natural ranking to search results based on confidence of predictions from the models and
   * is used when ANDing multiple of these types of RANK by Output queries together as well.
   * </pre>
   *
   * <code>.clarifai.api.Output output = 2;</code>
   */
  boolean hasOutput();
  /**
   * <pre>
   * RANK based predicted outputs from models such as custom trained models, pre-trained models,
   * etc. This is also where you enter the image url for a visual search because what we're asking
   * the system to do is find output embedding most visually similar to the provided input (that
   * input being in And.output.input.data.image.url for example). This will return the Hits
   * sorted by visual similarity (1.0 being very similar or exact match and 0.0 being very
   * dissimlar). For a search by Output concept, this means we're asking the system to rank
   * the Hits by confidence of our model's predicted Outputs. So for example if the model
   * predicts an image is 0.95 likely there is a "dog" present, that should related directly
   * to the score returned if you search for Output concept "dog" in your query. This provides
   * a natural ranking to search results based on confidence of predictions from the models and
   * is used when ANDing multiple of these types of RANK by Output queries together as well.
   * </pre>
   *
   * <code>.clarifai.api.Output output = 2;</code>
   */
  com.clarifai.grpc.api.Output getOutput();
  /**
   * <pre>
   * RANK based predicted outputs from models such as custom trained models, pre-trained models,
   * etc. This is also where you enter the image url for a visual search because what we're asking
   * the system to do is find output embedding most visually similar to the provided input (that
   * input being in And.output.input.data.image.url for example). This will return the Hits
   * sorted by visual similarity (1.0 being very similar or exact match and 0.0 being very
   * dissimlar). For a search by Output concept, this means we're asking the system to rank
   * the Hits by confidence of our model's predicted Outputs. So for example if the model
   * predicts an image is 0.95 likely there is a "dog" present, that should related directly
   * to the score returned if you search for Output concept "dog" in your query. This provides
   * a natural ranking to search results based on confidence of predictions from the models and
   * is used when ANDing multiple of these types of RANK by Output queries together as well.
   * </pre>
   *
   * <code>.clarifai.api.Output output = 2;</code>
   */
  com.clarifai.grpc.api.OutputOrBuilder getOutputOrBuilder();

  /**
   * <pre>
   * If True then this will flip the meaning of this part of the
   * query. This allow for queries such as dog AND ! metadata=={"blah":"value"}
   * </pre>
   *
   * <code>bool negate = 3;</code>
   */
  boolean getNegate();

  /**
   * <pre>
   * FILTER by annotation information. This is more flexible than just filtering by
   * Input information because in the general case each input can have several annotations.
   * Some example use cases for filtering by annotations:
   * 1) find all the inputs annotated "dog" by worker_id = "XYZ"
   * 2) find all the annotations associated with embed_model_version_id = "123"
   * 3) find all the annotations that are trusted, etc.
   * Since all the annotations under the hood are joined to the embedding model's annotation
   * using worker_id's of other models like cluster models or concept models should be
   * combinable with queries like visual search (a query with Output filled in).
   * </pre>
   *
   * <code>.clarifai.api.Annotation annotation = 4;</code>
   */
  boolean hasAnnotation();
  /**
   * <pre>
   * FILTER by annotation information. This is more flexible than just filtering by
   * Input information because in the general case each input can have several annotations.
   * Some example use cases for filtering by annotations:
   * 1) find all the inputs annotated "dog" by worker_id = "XYZ"
   * 2) find all the annotations associated with embed_model_version_id = "123"
   * 3) find all the annotations that are trusted, etc.
   * Since all the annotations under the hood are joined to the embedding model's annotation
   * using worker_id's of other models like cluster models or concept models should be
   * combinable with queries like visual search (a query with Output filled in).
   * </pre>
   *
   * <code>.clarifai.api.Annotation annotation = 4;</code>
   */
  com.clarifai.grpc.api.Annotation getAnnotation();
  /**
   * <pre>
   * FILTER by annotation information. This is more flexible than just filtering by
   * Input information because in the general case each input can have several annotations.
   * Some example use cases for filtering by annotations:
   * 1) find all the inputs annotated "dog" by worker_id = "XYZ"
   * 2) find all the annotations associated with embed_model_version_id = "123"
   * 3) find all the annotations that are trusted, etc.
   * Since all the annotations under the hood are joined to the embedding model's annotation
   * using worker_id's of other models like cluster models or concept models should be
   * combinable with queries like visual search (a query with Output filled in).
   * </pre>
   *
   * <code>.clarifai.api.Annotation annotation = 4;</code>
   */
  com.clarifai.grpc.api.AnnotationOrBuilder getAnnotationOrBuilder();
}
